# -*- coding: utf-8 -*-
"""HIT140-A3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1drg7zqE834Cj7ehmgSOzoChn2DhmDkcD
"""

# REVISED INVESTIGATION A USING LINEAR REGRESSION MODEL

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
import statsmodels.api as sm
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# LOADING

DATASET1 = "dataset1.csv" # bats
DATASET2 = "dataset2.csv" # rats

def parse_dt(series, primary_fmt="%d/%m/%Y %H:%M"):
    s = pd.to_datetime(series, format=primary_fmt, errors="coerce")
    if s.isna().mean() > 0.2:
        s = pd.to_datetime(series, errors="coerce", dayfirst=True, infer_datetime_format=True)
    return s

# Load datasets
d1 = pd.read_csv(DATASET1)
d2 = pd.read_csv(DATASET2)

# Parse date/time columns
d1["start_time"]       = parse_dt(d1.get("start_time"))
d1["rat_period_start"] = parse_dt(d1.get("rat_period_start"))
d1["rat_period_end"]   = parse_dt(d1.get("rat_period_end"))
d1["sunset_time"]      = parse_dt(d1.get("sunset_time"))
d2["time"]             = parse_dt(d2.get("time"))

# Merge on 30-minute intervals
d1["time_block"] = d1["start_time"].dt.floor("30min")
d2["time_block"] = d2["time"].dt.floor("30min")
merged = pd.merge(
    d1,
    d2[["time_block", "rat_minutes", "rat_arrival_number",
        "bat_landing_number", "food_availability"]],
    on="time_block",
    how="left"
)


# FEATURE ENGINEERING

# New explanatory variables for better description
merged["rat_presence"] = (merged["rat_minutes"] > 0).astype(int)
merged["rat_intensity"] = merged["rat_minutes"] * merged["rat_arrival_number"]
merged["bat_activity_rate"] = merged["bat_landing_number"] / (merged["rat_arrival_number"] + 1)
merged["food_pressure"] = merged["rat_minutes"] / (merged["food_availability"] + 1)
merged["hours_after_sunset_sq"] = merged["hours_after_sunset"] ** 2

# Defining response and features
response = "seconds_after_rat_arrival"
features = [
    "rat_minutes", "rat_arrival_number", "bat_landing_number",
    "food_availability", "hours_after_sunset", "rat_presence",
    "rat_intensity", "bat_activity_rate", "food_pressure", "hours_after_sunset_sq"
]

# Dropping missing or infinite values
data = merged[features + [response]].replace([np.inf, -np.inf], np.nan).dropna()
print(f"‚úÖ Data rows used for Linear Regression: {len(data)}")


# EXPLORE CORRELATIONS

plt.figure(figsize=(10,7))
sns.heatmap(data[features + [response]].corr(), annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Heatmap ‚Äì Bat vs Rat Behaviour Variables")
plt.tight_layout()
plt.savefig("heatmap_bat_rat_corr.png", dpi=300)
plt.close()


# Scale Features
scaler = StandardScaler()
scaled_X = scaler.fit_transform(data[features])
X_scaled = pd.DataFrame(scaled_X, columns=features)
X_scaled = sm.add_constant(X_scaled)
y = data[response]


# Fit Linear Regression Model

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

model = sm.OLS(y_train, X_train).fit()
print("\nüìä Linear Regression Summary (with Scaled Inputs):")
print(model.summary())


# Evaluate Model Performance

y_pred = model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("\nüìà Model Evaluation:")
print(f"R¬≤ Score: {r2:.3f}")
print(f"Mean Squared Error (MSE): {mse:.3f}")
print(f"Mean Absolute Error (MAE): {mae:.3f}")


# Visualise Predictions

plt.figure(figsize=(6,4))
plt.scatter(y_test, y_pred, alpha=0.6, edgecolor="violet")
plt.xlabel("Actual seconds_after_rat_arrival")
plt.ylabel("Predicted values")
plt.title("Predicted vs Actual (Scaled Linear Regression)")
plt.grid(True, linestyle="--", alpha=0.6)
plt.tight_layout()
plt.savefig("predicted_vs_actual_scaled.png", dpi=300)
plt.close()

print("\n‚úÖ Outputs saved: heatmap, model summary, evaluation metrics, and prediction plot.")

#OPTIMIZATION

# Keep only significant predictors (p < 0.05)
sig_vars = model.pvalues[model.pvalues < 0.05].index.tolist()
if "const" in sig_vars:
    sig_vars.remove("const")

if len(sig_vars) > 0:
    print(f"\n‚ú® Optimizing model: keeping significant predictors {sig_vars}")
    X_opt = sm.add_constant(X_scaled[sig_vars])
    model_opt = sm.OLS(y, X_opt).fit()
    print(model_opt.summary())

    # Evaluate optimized model
    y_opt_pred = model_opt.predict(X_opt)
    mse_opt = mean_squared_error(y, y_opt_pred)
    mae_opt = mean_absolute_error(y, y_opt_pred)
    r2_opt = r2_score(y, y_opt_pred)

    print("\nüîß Optimized Model Evaluation:")
    print(f"R¬≤: {r2_opt:.3f}, MSE: {mse_opt:.3f}, MAE: {mae_opt:.3f}")
else:
    print("\n‚ö†Ô∏è No predictors were statistically significant (p < 0.05). Model kept as is.")



#---------------------INVESTIGATION B-------------------------#

# Import necessary libraries

import statsmodels.formula.api as smf
import scipy.stats as stats
from statsmodels.stats.outliers_influence import variance_inflation_factor


def season_from_month(m):
    """Map month->season (text)"""
    if m in [12, 1, 2]:
        return "winter"
    elif m in [3, 4, 5]:
        return "spring"
    elif m in [6, 7, 8]:
        return "summer"
    elif m in [9, 10, 11]:
        return "autumn"
    else:
        return None

def safe_div(a, b):
    """Safely divide values."""
    with np.errstate(divide='ignore', invalid='ignore'):
        out = np.where(b != 0, a / b, np.nan)
    return out

def zscore(series):
    """Standardize features to mean=0, std=1."""
    return (series - series.mean()) / (series.std(ddof=0) + 1e-9)

def compute_vif(X_df):
    """Compute Variance Inflation Factor for features."""
    X = sm.add_constant(X_df, has_constant='add')
    vif = []
    for i, col in enumerate(X.columns):
        try:
            vif_val = variance_inflation_factor(X.values, i)
        except Exception:
            vif_val = np.nan
        vif.append((col, vif_val))
    return pd.DataFrame(vif, columns=["variable", "VIF"])

# Load datasets
df1 = pd.read_csv("dataset1.csv")  # Bat dataset
df2 = pd.read_csv("dataset2.csv")  # Rat dataset

# Print basic info
print("Dataset 1 columns:", df1.columns.tolist())
print("Dataset 2 columns:", df2.columns.tolist())

# Parse datetime columns
df1["start_time"] = parse_dt(df1["start_time"])
df1["rat_period_start"] = parse_dt(df1["rat_period_start"])
df1["rat_period_end"] = parse_dt(df1["rat_period_end"])
df1["sunset_time"] = parse_dt(df1["sunset_time"])
df2["time"] = parse_dt(df2["time"])

# Merge on 30-minute intervals
df1["time_block"] = df1["start_time"].dt.floor("30min")
df2["time_block"] = df2["time"].dt.floor("30min")
merged = pd.merge(df1, df2[["time_block", "rat_minutes", "rat_arrival_number", "bat_landing_number", "food_availability"]],
                  on="time_block", how="left")

# Feature Engineering
merged["rat_presence"] = (merged["rat_minutes"] > 0).astype(int)
merged["rat_intensity"] = merged["rat_minutes"] * merged["rat_arrival_number"]
merged["bat_activity_rate"] = merged["bat_landing_number"] / (merged["rat_arrival_number"] + 1)
merged["food_pressure"] = merged["rat_minutes"] / (merged["food_availability"] + 1)
merged["hours_after_sunset_sq"] = merged["hours_after_sunset"] ** 2

# Add season based on month
if "month" in merged.columns:
    merged["month"] = pd.to_numeric(merged["month"], errors="coerce")
    merged["season"] = merged["month"].apply(season_from_month)
else:
    print("Warning: 'month' column not found in merged dataset.")

response_var = "seconds_after_rat_arrival"
explanatory_vars = ["rat_minutes", "rat_arrival_number", "bat_landing_number", "food_availability", "hours_after_sunset",
                    "rat_presence", "rat_intensity", "bat_activity_rate", "food_pressure", "hours_after_sunset_sq"]


# Visualize seasonal counts if 'season' column exists
if "season" in merged.columns:
    season_counts = merged["season"].value_counts(dropna=False)
    plt.figure(figsize=(6, 4))
    season_counts.plot(kind="bar", color="skyblue")
    plt.title("Season Count Distribution")
    plt.xlabel("Season")
    plt.ylabel("Count")
    plt.tight_layout()
    plt.show()

# Descriptive Statistics: Central Tendency and Dispersion
print("Descriptive Statistics for Numerical Columns:")
desc_stats = merged.describe(include="all")
print(desc_stats)

# Visualize the distribution of the key variables using histograms
plt.figure(figsize=(12, 8))
for i, var in enumerate(["rat_minutes", "rat_arrival_number", "bat_landing_number", "food_availability", "hours_after_sunset"]):
    plt.subplot(2, 3, i+1)
    sns.histplot(merged[var], kde=True, color='skyblue')
    plt.title(f"Distribution of {var}")
    plt.xlabel(var)
    plt.ylabel("Frequency")
plt.tight_layout()
plt.show()

# Visualize the correlation heatmap between explanatory variables
correlation_matrix = merged[explanatory_vars + [response_var]].corr()
plt.figure(figsize=(10, 7))
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.title("Correlation Heatmap ‚Äì Explanatory Variables and Response", fontsize=16)
plt.tight_layout()
plt.show()

# Seasonal Breakdown: Summarize data by season
season_summary = merged.groupby("season")[explanatory_vars + [response_var]].describe().transpose()
print("\nSeasonal Summary Statistics:")
print(season_summary)

# Visualize the distribution of the response variable by season
plt.figure(figsize=(8, 5))
sns.violinplot(x="season", y=response_var, data=merged, palette="muted")
plt.title(f"Violin Plot of {response_var} by Season", fontsize=16)
plt.xlabel("Season")
plt.ylabel(response_var)
plt.tight_layout()
plt.show()

# Boxplot to visualize the distribution of explanatory variables across seasons
plt.figure(figsize=(12, 8))
for i, var in enumerate(explanatory_vars):
    plt.subplot(3, 4, i+1)
    sns.boxplot(x="season", y=var, data=merged, palette="muted")
    plt.title(f"Seasonal Distribution of {var}")
    plt.xlabel("Season")
    plt.ylabel(var)
plt.tight_layout()
plt.show()

# Descriptive summary of missing values (if any)
missing_values = merged.isnull().sum()
missing_percentage = (missing_values / len(merged)) * 100
missing_summary = pd.DataFrame({"Missing Values": missing_values, "Percentage": missing_percentage})
print("\nMissing Values Summary:")
print(missing_summary)


# Separate for Winter and Spring
winter_df = merged[merged["season"] == "winter"]
spring_df = merged[merged["season"] == "spring"]

# Feature and target variable setup
response_var = "seconds_after_rat_arrival"
explanatory_vars = ["rat_minutes", "rat_arrival_number", "bat_landing_number", "food_availability", "hours_after_sunset",
                    "rat_presence", "rat_intensity", "bat_activity_rate", "food_pressure", "hours_after_sunset_sq"]

# Cleaning missing values
merged = merged.dropna(subset=[response_var] + explanatory_vars)
print(f"Cleaned dataset rows: {len(merged)}")

# Standardize the explanatory variables
scaler = StandardScaler()
X = scaler.fit_transform(merged[explanatory_vars])
X = sm.add_constant(X)
y = merged[response_var]

# Split data for model training
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Fit Linear Regression Model
model = sm.OLS(y_train, X_train).fit()
print(model.summary())

# Predictions and evaluation
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("\nModel Evaluation")
print(f"R¬≤ Score: {r2:.3f}")
print(f"Mean Squared Error (MSE): {mse:.3f}")
print(f"Mean Absolute Error (MAE): {mae:.3f}")

# Visualize Predicted vs Actual
plt.figure(figsize=(6, 4))
plt.scatter(y_test, y_pred, alpha=0.6, edgecolor="violet")
plt.xlabel("Actual values")
plt.ylabel("Predicted values")
plt.title("Predicted vs Actual")
plt.grid(True, linestyle="--", alpha=0.6)
plt.tight_layout()
plt.show()

# VIF (Multicollinearity check)
X_df = merged[explanatory_vars]
vif_df = compute_vif(X_df)
print("VIF for features:")
print(vif_df)

# Residual Analysis
residuals = y_test - y_pred
plt.figure(figsize=(6, 4))
sns.histplot(residuals, kde=True, color="red", alpha=0.5)
plt.title("Residual Distribution")
plt.xlabel("Residuals")
plt.tight_layout()
plt.show()

# Season Interaction Model (Fixed Formula)
if "season" in merged.columns:
    merged["season"] = merged["season"].astype("category")

    # Fix the formula construction
    formula = f"{response_var} ~ " + " + ".join(explanatory_vars) + " + season + " + " + ".join([f"{var}:season" for var in explanatory_vars])

    # Fit the model
    season_model = smf.ols(formula, data=merged).fit()
    print(season_model.summary())
else:
    print("Season column is missing, skipping the Season Interaction Model.")


# Correlation Heatmap
plt.figure(figsize=(10, 7))
sns.heatmap(merged[explanatory_vars + [response_var]].corr(), annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.title("Correlation Heatmap ‚Äì Explanatory Variables and Response", fontsize=16)
plt.tight_layout()
plt.show()

# Residual Plot
plt.figure(figsize=(8, 5))
sns.residplot(x=season_model.fittedvalues, y=season_model.resid, lowess=True, line_kws={'color': 'red'})
plt.title("Residuals vs Fitted Values", fontsize=16)
plt.xlabel("Fitted Values")
plt.ylabel("Residuals")
plt.tight_layout()
plt.show()